{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook runs optimization experiments on different combination of input/target normalization image transforms with fixed training hyper parameters to inform decision of what normalization method to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sys\n",
    "import yaml\n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import mlflow\n",
    "import optuna\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pathlib.Path('.').absolute().parent.parent / \"config.yml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import virtual_stain_flow software "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/weishanli/Waylab/pediatric_cancer_atlas_analysis\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(config['paths']['software_path'])\n",
    "print(str(pathlib.Path('.').absolute().parent.parent))\n",
    "\n",
    "## Dataset\n",
    "from virtual_stain_flow.datasets.PatchDataset import PatchDataset\n",
    "from virtual_stain_flow.datasets.CachedDataset import CachedDataset\n",
    "\n",
    "## FNet training\n",
    "from virtual_stain_flow.models.fnet import FNet\n",
    "from virtual_stain_flow.trainers.Trainer import Trainer\n",
    "\n",
    "from virtual_stain_flow.transforms.MinMaxNormalize import MinMaxNormalize\n",
    "from virtual_stain_flow.transforms.PixelDepthTransform import PixelDepthTransform\n",
    "from virtual_stain_flow.transforms.ZScoreNormalize import ZScoreNormalize\n",
    "\n",
    "## Metrics\n",
    "from virtual_stain_flow.metrics.MetricsWrapper import MetricsWrapper\n",
    "from virtual_stain_flow.metrics.PSNR import PSNR\n",
    "from virtual_stain_flow.metrics.SSIM import SSIM\n",
    "\n",
    "## callback\n",
    "from virtual_stain_flow.callbacks.MlflowLogger import MlflowLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define paths and other train parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loaddata for train\n",
    "LOADDATA_FILE_PATH = pathlib.Path('.').absolute().parent.parent \\\n",
    "    / '0.data_preprocessing' / 'data_split_loaddata' / 'loaddata_train.csv'\n",
    "assert LOADDATA_FILE_PATH.exists()\n",
    "SC_FEATURES_DIR = pathlib.Path(config['paths']['sc_features_path'])\n",
    "\n",
    "## Output directories\n",
    "MLFLOW_DIR = pathlib.Path('.').absolute() / 'optuna_mlflow'\n",
    "MLFLOW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OPTUNA_JOBLIB_DIR = pathlib.Path('.').absolute() / 'optuna_joblib'\n",
    "OPTUNA_JOBLIB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "## Basic data generation, model convolutional depth, optimizer param and max epoch definition\n",
    "PATCH_SIZE = 256\n",
    "CONV_DEPTH = 5\n",
    "LR = 1e-4\n",
    "BETAS = (0.5, 0.9)\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 1_000\n",
    "PATIENCE = 20\n",
    "\n",
    "## Channels for input and target are read from config\n",
    "INPUT_CHANNEL_NAMES = config['data']['input_channel_keys']\n",
    "TARGET_CHANNEL_NAMES = config['data']['target_channel_keys']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Normalization Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define transforms and parameters\n",
    "NORM_METHODS = {\n",
    "    \"z_score\": {\n",
    "        \"class\": ZScoreNormalize,\n",
    "        \"args\": {\"_mean\": None, \"_std\": None, \"_always_apply\": True, \"_p\": 1.0}\n",
    "    },\n",
    "    \"8bit\": {\n",
    "        \"class\": PixelDepthTransform,\n",
    "        \"args\": {\"src_bit_depth\": 16, \"target_bit_depth\": 8, \"_always_apply\": True, \"_p\": 1.0}\n",
    "    },\n",
    "    \"min_max\": {\n",
    "        \"class\": MinMaxNormalize,\n",
    "        \"args\": {\"_normalization_factor\": (2 ** 16) - 1, \"_always_apply\": True, \"_p\": 1.0}\n",
    "    }\n",
    "}\n",
    "\n",
    "## Define the model output activation to be used with each output normalization\n",
    "NORM_METHOD_ACTIVATION = {\n",
    "    \"z_score\": \"linear\",\n",
    "    \"8bit\": \"linear\",\n",
    "    \"min_max\": \"sigmoid\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define optimization objective functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def free_gpu_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def objective(trial, dataset, channel_name):\n",
    "\n",
    "    # Suggest an input and targettransform\n",
    "    input_transform = trial.suggest_categorical(\"input_transform\", list(NORM_METHODS.keys()))\n",
    "    target_transform = trial.suggest_categorical(\"target_transform\", list(NORM_METHODS.keys()))\n",
    "\n",
    "    ## Configure the dataset with normalization methods\n",
    "    dataset.set_input_transform(NORM_METHODS[input_transform][\"class\"](**NORM_METHODS[input_transform][\"args\"]))\n",
    "    dataset.set_target_transform(NORM_METHODS[target_transform][\"class\"](**NORM_METHODS[target_transform][\"args\"]))\n",
    "\n",
    "    ## Cache dataset\n",
    "    cached_dataset = CachedDataset(\n",
    "            dataset=dataset,\n",
    "            prefill_cache=True\n",
    "        )\n",
    "\n",
    "    ## Setup model and optimizer\n",
    "    model = FNet(depth=CONV_DEPTH, \n",
    "                 # output activation paired with target/output normalization\n",
    "                 output_activation=NORM_METHOD_ACTIVATION[target_transform])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR, betas=BETAS)\n",
    "    \n",
    "    ## Metrics to be computed (and logged)\n",
    "    metric_fns = {\n",
    "        \"mse_loss\": MetricsWrapper(_metric_name='mse', module=torch.nn.MSELoss()),\n",
    "        \"ssim_loss\": SSIM(_metric_name=\"ssim\"),\n",
    "        \"psnr_loss\": PSNR(_metric_name=\"psnr\"),\n",
    "    }\n",
    "\n",
    "    ## Params to log with mlflow\n",
    "    params = {\n",
    "            \"lr\": LR,\n",
    "            \"beta0\": BETAS[0],\n",
    "            \"beta1\": BETAS[1],\n",
    "            \"depth\": CONV_DEPTH,\n",
    "            \"patch_size\": PATCH_SIZE,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"epochs\": EPOCHS,\n",
    "            \"patience\": PATIENCE,\n",
    "            \"input_norm\": input_transform,\n",
    "            \"target_norm\": target_transform,\n",
    "            \"channel_name\": channel_name,\n",
    "        }\n",
    "\n",
    "    ## mlflow logger callback\n",
    "    mlflow_logger_callback = MlflowLogger(\n",
    "        name='mlflow_logger',\n",
    "        mlflow_uri=MLFLOW_DIR / 'mlruns',\n",
    "        mlflow_experiment_name='FNet_optimize_io_norm',\n",
    "        mlflow_start_run_args={'run_name': f'FNet_optimize_io_norm_{channel_name}', 'nested': True},\n",
    "        mlflow_log_params_args=params\n",
    "    )\n",
    "    \n",
    "    ## Trainer\n",
    "    trainer = Trainer(\n",
    "        model = model,\n",
    "        optimizer = optimizer,\n",
    "        backprop_loss = torch.nn.L1Loss(), # MAE loss for backpropagation\n",
    "        dataset = cached_dataset,\n",
    "        batch_size = BATCH_SIZE,\n",
    "        epochs = EPOCHS,\n",
    "        patience = PATIENCE,\n",
    "        callbacks=[mlflow_logger_callback],\n",
    "        metrics=metric_fns,\n",
    "        device = 'cuda',\n",
    "        early_termination_metric='L1Loss'\n",
    "    )\n",
    "\n",
    "    # Train the model and log validation loss\n",
    "    trainer.train()\n",
    "    val_loss = trainer.best_loss\n",
    "\n",
    "    del model\n",
    "    del optimizer\n",
    "    del metric_fns\n",
    "    del mlflow_logger_callback\n",
    "    del trainer\n",
    "    \n",
    "    free_gpu_memory()\n",
    "\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-21 16:02:53,938] A new study created in memory with name: FNet_optimize_OrigDNA_io_norm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning optimization for channel: OrigDNA for io normalization methods\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/21 16:02:55 INFO mlflow.tracking.fluent: Experiment with name 'FNet_optimize_io_norm' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early termination at epoch 98 with best validation metric 0.21405574679374695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-21 16:06:21,336] Trial 0 finished with value: 0.21405574679374695 and parameters: {'input_transform': '8bit', 'target_transform': 'z_score'}. Best is trial 0 with value: 0.21405574679374695.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early termination at epoch 75 with best validation metric 0.00727159483358264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-21 16:08:56,865] Trial 1 finished with value: 0.00727159483358264 and parameters: {'input_transform': '8bit', 'target_transform': 'min_max'}. Best is trial 1 with value: 0.00727159483358264.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early termination at epoch 74 with best validation metric 0.006408152868971229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-21 16:11:30,864] Trial 2 finished with value: 0.006408152868971229 and parameters: {'input_transform': 'z_score', 'target_transform': 'min_max'}. Best is trial 2 with value: 0.006408152868971229.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early termination at epoch 71 with best validation metric 0.007854564930312335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-21 16:14:00,045] Trial 3 finished with value: 0.007854564930312335 and parameters: {'input_transform': 'min_max', 'target_transform': 'min_max'}. Best is trial 2 with value: 0.006408152868971229.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early termination at epoch 147 with best validation metric 1.6219418346881866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-21 16:18:58,717] Trial 4 finished with value: 1.6219418346881866 and parameters: {'input_transform': '8bit', 'target_transform': '8bit'}. Best is trial 2 with value: 0.006408152868971229.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early termination at epoch 81 with best validation metric 1.6017951667308807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-21 16:21:45,978] Trial 5 finished with value: 1.6017951667308807 and parameters: {'input_transform': 'z_score', 'target_transform': '8bit'}. Best is trial 2 with value: 0.006408152868971229.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early termination at epoch 77 with best validation metric 0.006991555797867477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-21 16:24:24,586] Trial 6 finished with value: 0.006991555797867477 and parameters: {'input_transform': 'min_max', 'target_transform': 'min_max'}. Best is trial 2 with value: 0.006408152868971229.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early termination at epoch 58 with best validation metric 0.21389151364564896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-21 16:26:23,131] Trial 7 finished with value: 0.21389151364564896 and parameters: {'input_transform': '8bit', 'target_transform': 'z_score'}. Best is trial 2 with value: 0.006408152868971229.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early termination at epoch 53 with best validation metric 1.4986083805561066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-21 16:28:13,029] Trial 8 finished with value: 1.4986083805561066 and parameters: {'input_transform': 'min_max', 'target_transform': '8bit'}. Best is trial 2 with value: 0.006408152868971229.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early termination at epoch 71 with best validation metric 0.007429587189108133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-21 16:30:42,855] Trial 9 finished with value: 0.007429587189108133 and parameters: {'input_transform': '8bit', 'target_transform': 'min_max'}. Best is trial 2 with value: 0.006408152868971229.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early termination at epoch 66 with best validation metric 0.006760006071999669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-21 16:33:00,082] Trial 10 finished with value: 0.006760006071999669 and parameters: {'input_transform': 'z_score', 'target_transform': 'min_max'}. Best is trial 2 with value: 0.006408152868971229.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early termination at epoch 55 with best validation metric 0.007510633091442287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-21 16:34:55,894] Trial 11 finished with value: 0.007510633091442287 and parameters: {'input_transform': 'z_score', 'target_transform': 'min_max'}. Best is trial 2 with value: 0.006408152868971229.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early termination at epoch 60 with best validation metric 0.006767947343178093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-21 16:38:06,881] Trial 12 finished with value: 0.006767947343178093 and parameters: {'input_transform': 'z_score', 'target_transform': 'min_max'}. Best is trial 2 with value: 0.006408152868971229.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early termination at epoch 53 with best validation metric 0.009068837389349937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-21 16:40:04,656] Trial 13 finished with value: 0.009068837389349937 and parameters: {'input_transform': 'z_score', 'target_transform': 'min_max'}. Best is trial 2 with value: 0.006408152868971229.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early termination at epoch 93 with best validation metric 0.006672206800431013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-21 16:43:30,500] Trial 14 finished with value: 0.006672206800431013 and parameters: {'input_transform': 'z_score', 'target_transform': 'min_max'}. Best is trial 2 with value: 0.006408152868971229.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early termination at epoch 62 with best validation metric 0.23114794865250587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-21 16:45:47,131] Trial 15 finished with value: 0.23114794865250587 and parameters: {'input_transform': 'z_score', 'target_transform': 'z_score'}. Best is trial 2 with value: 0.006408152868971229.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early termination at epoch 83 with best validation metric 0.006946340203285217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-21 16:48:49,735] Trial 16 finished with value: 0.006946340203285217 and parameters: {'input_transform': 'z_score', 'target_transform': 'min_max'}. Best is trial 2 with value: 0.006408152868971229.\n"
     ]
    }
   ],
   "source": [
    "RUN_EXTRA = False\n",
    "\n",
    "## Load dataset\n",
    "loaddata_df = pd.read_csv(LOADDATA_FILE_PATH)\n",
    "sc_features = pd.DataFrame()\n",
    "for plate in loaddata_df['Metadata_Plate'].unique():\n",
    "    sc_features_parquet = SC_FEATURES_DIR / f'{plate}_sc_normalized.parquet'\n",
    "    if not sc_features_parquet.exists():\n",
    "        print(f'{sc_features_parquet} does not exist, skipping...')\n",
    "        continue \n",
    "    else:\n",
    "        sc_features = pd.concat([\n",
    "            sc_features, \n",
    "            pd.read_parquet(\n",
    "                sc_features_parquet,\n",
    "                columns=['Metadata_Plate', 'Metadata_Well', 'Metadata_Site', 'Metadata_Cells_Location_Center_X', 'Metadata_Cells_Location_Center_Y']\n",
    "            )\n",
    "        ])\n",
    "\n",
    "pds = PatchDataset(\n",
    "        _loaddata_csv=loaddata_df,\n",
    "        _sc_feature=sc_features,\n",
    "        _input_channel_keys=INPUT_CHANNEL_NAMES,\n",
    "        _target_channel_keys=TARGET_CHANNEL_NAMES,\n",
    "        _input_transform=None,\n",
    "        _target_transform=None,\n",
    "        patch_size=PATCH_SIZE,\n",
    "        verbose=False,\n",
    "        patch_generation_method=\"random_cell\",\n",
    "        n_expected_patches_per_img=50,\n",
    "        patch_generation_random_seed=42\n",
    "    )\n",
    "\n",
    "for channel_name in TARGET_CHANNEL_NAMES:\n",
    "\n",
    "    ## Configure dataset channel\n",
    "    pds.set_input_channel_keys(INPUT_CHANNEL_NAMES)\n",
    "    pds.set_target_channel_keys(channel_name)\n",
    "\n",
    "    print(f\"Beginning optimization for channel: {channel_name} for io normalization methods\")\n",
    "\n",
    "    # Load the existing study for the current channel\n",
    "    study_path = OPTUNA_JOBLIB_DIR / f\"FNet_optimize_{channel_name}_io_norm.joblib\"\n",
    "    if study_path.exists():\n",
    "        if RUN_EXTRA:\n",
    "            study = joblib.load(study_path)\n",
    "        else:\n",
    "            print(\"Skipping optimization due to existing joblib...\")\n",
    "            continue\n",
    "    else:\n",
    "        study = optuna.create_study(\n",
    "            direction=\"minimize\",\n",
    "            study_name=f\"FNet_optimize_{channel_name}_io_norm\",\n",
    "            sampler=optuna.samplers.TPESampler(seed=42)\n",
    "        )\n",
    "\n",
    "    study.optimize(lambda trial: objective(trial, pds, channel_name), n_trials=50)\n",
    "    \n",
    "    # Save the updated study for the current channel\n",
    "    joblib.dump(study, study_path)\n",
    "\n",
    "    # Print best trial results\n",
    "    print(f\"Best trial for channel {channel_name}:\")\n",
    "    print(f\"  Validation Loss: {study.best_trial.value}\")\n",
    "    print(f\"  Hyperparameters: {study.best_trial.params}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speckle_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
