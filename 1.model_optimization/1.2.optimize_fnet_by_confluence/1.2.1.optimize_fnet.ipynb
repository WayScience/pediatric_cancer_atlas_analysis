{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import mlflow\n",
    "import optuna\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pathlib.Path('.').absolute().parent.parent / \"config.yml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import virtual_stain_flow software "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/weishanli/Waylab/pediatric_cancer_atlas_analysis\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(config['paths']['software_path'])\n",
    "print(str(pathlib.Path('.').absolute().parent.parent))\n",
    "\n",
    "## Dataset\n",
    "from virtual_stain_flow.datasets.PatchDataset import PatchDataset\n",
    "from virtual_stain_flow.datasets.CachedDataset import CachedDataset\n",
    "\n",
    "## FNet training\n",
    "from virtual_stain_flow.models.fnet import FNet\n",
    "from virtual_stain_flow.trainers.Trainer import Trainer\n",
    "\n",
    "## wGaN training\n",
    "from virtual_stain_flow.models.unet import UNet\n",
    "from virtual_stain_flow.models.discriminator import GlobalDiscriminator\n",
    "from virtual_stain_flow.trainers.WGaNTrainer import WGaNTrainer\n",
    "\n",
    "## wGaN losses\n",
    "from virtual_stain_flow.losses.GradientPenaltyLoss import GradientPenaltyLoss\n",
    "from virtual_stain_flow.losses.DiscriminatorLoss import DiscriminatorLoss\n",
    "from virtual_stain_flow.losses.GeneratorLoss import GeneratorLoss\n",
    "\n",
    "from virtual_stain_flow.transforms.MinMaxNormalize import MinMaxNormalize\n",
    "from virtual_stain_flow.transforms.PixelDepthTransform import PixelDepthTransform\n",
    "\n",
    "## Metrics\n",
    "from virtual_stain_flow.metrics.MetricsWrapper import MetricsWrapper\n",
    "from virtual_stain_flow.metrics.PSNR import PSNR\n",
    "from virtual_stain_flow.metrics.SSIM import SSIM\n",
    "\n",
    "## callback\n",
    "from virtual_stain_flow.callbacks.MlflowLogger import MlflowLogger\n",
    "from virtual_stain_flow.callbacks.IntermediatePlot import IntermediatePatchPlot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define paths and other train parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loaddata for train\n",
    "LOADDATA_FILE_PATH = pathlib.Path('.').absolute().parent.parent \\\n",
    "    / '0.data_preprocessing' / 'data_split_loaddata' / 'loaddata_train.csv'\n",
    "assert LOADDATA_FILE_PATH.exists()\n",
    "SC_FEATURES_DIR = pathlib.Path(config['paths']['sc_features_path'])\n",
    "\n",
    "## Output directories\n",
    "MLFLOW_DIR = pathlib.Path('.').absolute() / 'optuna_mlflow'\n",
    "MLFLOW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OPTUNA_JOBLIB_DIR = pathlib.Path('.').absolute() / 'optuna_joblib'\n",
    "OPTUNA_JOBLIB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "## Basic data generation, model convolutional depth and max epoch definition\n",
    "PATCH_SIZE = 256\n",
    "DEPTH = 5\n",
    "EPOCHS = 1_000\n",
    "\n",
    "## Channels for input and target are read from config\n",
    "INPUT_CHANNEL_NAMES = config['data']['input_channel_keys']\n",
    "TARGET_CHANNEL_NAMES = config['data']['target_channel_keys']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defines how the train data will be divided to train models on two levels of confluence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_GROUPING = {\n",
    "    'high_confluence': {\n",
    "        'seeding_density': [12_000, 8_000]\n",
    "    },\n",
    "    'low_confluence': {\n",
    "        'seeding_density': [4_000, 2_000, 1_000]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define optimization objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def free_gpu_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def objective(trial, dataset, channel_name, confluence_group_name):\n",
    "    \n",
    "    ## Suggest hyperparameters\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True) # adam optimizer learning rate\n",
    "    beta0 = trial.suggest_float('beta0', 0.5, 0.9) # adam optimizer beta 0\n",
    "    beta1 = trial.suggest_float('beta1', 0.9, 0.999) # adam optimizer beta 1\n",
    "    betas = (beta0, beta1)\n",
    "\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 64, step=16)\n",
    "    patience = trial.suggest_int('patience', 5, 20) # early stop patience\n",
    "\n",
    "    conv_depth = trial.suggest_int('conv_depth', 3, 5) # convolutional depth for fnet model\n",
    "\n",
    "    ## Setup model and optimizer\n",
    "    model = FNet(depth=conv_depth)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, betas=betas)\n",
    "    \n",
    "    ## Metrics to be computed (and logged)\n",
    "    metric_fns = {\n",
    "        \"mse_loss\": MetricsWrapper(_metric_name='mse', module=torch.nn.MSELoss()),\n",
    "        \"ssim_loss\": SSIM(_metric_name=\"ssim\"),\n",
    "        \"psnr_loss\": PSNR(_metric_name=\"psnr\"),\n",
    "    }\n",
    "\n",
    "    ## Params to log with mlflow\n",
    "    params = {\n",
    "            \"lr\": lr,\n",
    "            \"beta0\": beta0,\n",
    "            \"beta1\": beta1,\n",
    "            \"depth\": conv_depth,\n",
    "            \"patch_size\": PATCH_SIZE,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"epochs\": EPOCHS,\n",
    "            \"patience\": patience,\n",
    "            \"channel_name\": channel_name,\n",
    "        }\n",
    "\n",
    "    ## mlflow logger callback\n",
    "    mlflow_logger_callback = MlflowLogger(\n",
    "        name='mlflow_logger',\n",
    "        mlflow_uri=MLFLOW_DIR / 'mlruns',\n",
    "        mlflow_experiment_name=f'FNet_optimize_{confluence_group_name}',\n",
    "        mlflow_start_run_args={'run_name': f'FNet_optimize_{confluence_group_name}_{channel_name}', 'nested': True},\n",
    "        mlflow_log_params_args=params\n",
    "    )\n",
    "    \n",
    "    ## Trainer\n",
    "    trainer = Trainer(\n",
    "        model = model,\n",
    "        optimizer = optimizer,\n",
    "        backprop_loss = torch.nn.L1Loss(), # MAE loss for backpropagation\n",
    "        dataset = dataset,\n",
    "        batch_size = batch_size,\n",
    "        epochs = EPOCHS,\n",
    "        patience = patience,\n",
    "        callbacks=[mlflow_logger_callback],\n",
    "        metrics=metric_fns,\n",
    "        device = 'cuda',\n",
    "        early_termination_metric='L1Loss'\n",
    "    )\n",
    "\n",
    "    # Train the model and log validation loss\n",
    "    trainer.train()\n",
    "    val_loss = trainer.best_loss\n",
    "\n",
    "    del model\n",
    "    del optimizer\n",
    "    del metric_fns\n",
    "    del mlflow_logger_callback\n",
    "    del trainer\n",
    "    \n",
    "    free_gpu_memory()\n",
    "\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning optimization for channel: OrigDNA for high_confluence\n",
      "Best trial for channel OrigDNA:\n",
      "  Validation Loss: 0.016428549215197562\n",
      "  Hyperparameters: {'lr': 0.0027501521311678614, 'beta0': 0.8688225992072988, 'beta1': 0.9654164885801781, 'batch_size': 48, 'patience': 16, 'conv_depth': 4}\n",
      "Beginning optimization for channel: OrigER for high_confluence\n",
      "Best trial for channel OrigER:\n",
      "  Validation Loss: 0.020866319803254946\n",
      "  Hyperparameters: {'lr': 0.00044118732609755304, 'beta0': 0.8125413329356658, 'beta1': 0.9146557494825732, 'batch_size': 32, 'patience': 12, 'conv_depth': 3}\n",
      "Beginning optimization for channel: OrigAGP for high_confluence\n",
      "Best trial for channel OrigAGP:\n",
      "  Validation Loss: 0.0029262731383953777\n",
      "  Hyperparameters: {'lr': 0.0005690482786963295, 'beta0': 0.8951555358866697, 'beta1': 0.9770467004930141, 'batch_size': 32, 'patience': 15, 'conv_depth': 4}\n",
      "Beginning optimization for channel: OrigMito for high_confluence\n",
      "Best trial for channel OrigMito:\n",
      "  Validation Loss: 0.06562353727909234\n",
      "  Hyperparameters: {'lr': 8.989081354219003e-05, 'beta0': 0.6470384526146977, 'beta1': 0.9467986866591369, 'batch_size': 16, 'patience': 11, 'conv_depth': 3}\n",
      "Beginning optimization for channel: OrigRNA for high_confluence\n",
      "Early termination at epoch 21 with best validation metric 0.019931084609457424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-24 09:50:30,158] Trial 30 finished with value: 0.019931084609457424 and parameters: {'lr': 0.0003023080480977387, 'beta0': 0.6641587946853241, 'beta1': 0.9619633462077796, 'batch_size': 32, 'patience': 8, 'conv_depth': 4}. Best is trial 22 with value: 0.018208095517296057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved study after trial 31/50\n",
      "Early termination at epoch 21 with best validation metric 0.018611485425096292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-24 09:52:54,020] Trial 31 finished with value: 0.018611485425096292 and parameters: {'lr': 0.0002887756125661432, 'beta0': 0.6079006601570802, 'beta1': 0.9520948147300842, 'batch_size': 16, 'patience': 6, 'conv_depth': 4}. Best is trial 22 with value: 0.018208095517296057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved study after trial 32/50\n",
      "Early termination at epoch 16 with best validation metric 0.019879762942974385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-24 09:54:42,258] Trial 32 finished with value: 0.019879762942974385 and parameters: {'lr': 0.00045042212292309175, 'beta0': 0.5264939038178491, 'beta1': 0.953639165228805, 'batch_size': 16, 'patience': 6, 'conv_depth': 4}. Best is trial 22 with value: 0.018208095517296057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved study after trial 33/50\n",
      "Early termination at epoch 26 with best validation metric 0.019488041360790912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-24 09:57:33,510] Trial 33 finished with value: 0.019488041360790912 and parameters: {'lr': 0.0001968865394686453, 'beta0': 0.6063412196687387, 'beta1': 0.9447814732621563, 'batch_size': 16, 'patience': 5, 'conv_depth': 4}. Best is trial 22 with value: 0.018208095517296057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved study after trial 34/50\n",
      "Early termination at epoch 31 with best validation metric 0.02039381116628647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-24 10:00:59,116] Trial 34 finished with value: 0.02039381116628647 and parameters: {'lr': 7.41466776056484e-05, 'beta0': 0.6378665015751696, 'beta1': 0.9585799496583057, 'batch_size': 16, 'patience': 9, 'conv_depth': 4}. Best is trial 22 with value: 0.018208095517296057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved study after trial 35/50\n",
      "Early termination at epoch 17 with best validation metric 0.01902948721097066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-24 10:02:54,162] Trial 35 finished with value: 0.01902948721097066 and parameters: {'lr': 0.0002388932800400586, 'beta0': 0.5841576203574859, 'beta1': 0.9693987250418548, 'batch_size': 16, 'patience': 6, 'conv_depth': 4}. Best is trial 22 with value: 0.018208095517296057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved study after trial 36/50\n",
      "Early termination at epoch 24 with best validation metric 0.019311377492088538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-24 10:05:54,404] Trial 36 finished with value: 0.019311377492088538 and parameters: {'lr': 0.00014902646085335333, 'beta0': 0.5622147649221322, 'beta1': 0.9782604091267229, 'batch_size': 16, 'patience': 8, 'conv_depth': 5}. Best is trial 22 with value: 0.018208095517296057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved study after trial 37/50\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import optuna\n",
    "import pandas as pd\n",
    "\n",
    "N_TRIALS = 50  # Total trials\n",
    "loaddata_df = pd.read_csv(LOADDATA_FILE_PATH)\n",
    "\n",
    "for confluence_group_name, conditions in DATA_GROUPING.items():\n",
    "\n",
    "    ## Load dataset\n",
    "    loaddata_condition_df = loaddata_df.copy()\n",
    "    for condition, values in conditions.items():\n",
    "        loaddata_condition_df = loaddata_condition_df[\n",
    "            loaddata_condition_df[condition].isin(values)\n",
    "        ]\n",
    "\n",
    "    sc_features = pd.DataFrame()\n",
    "    for plate in loaddata_condition_df['Metadata_Plate'].unique():\n",
    "        sc_features_parquet = SC_FEATURES_DIR / f'{plate}_sc_normalized.parquet'\n",
    "        if not sc_features_parquet.exists():\n",
    "            print(f'{sc_features_parquet} does not exist, skipping...')\n",
    "            continue \n",
    "        else:\n",
    "            sc_features = pd.concat([\n",
    "                sc_features, \n",
    "                pd.read_parquet(\n",
    "                    sc_features_parquet,\n",
    "                    columns=['Metadata_Plate', 'Metadata_Well', 'Metadata_Site', 'Metadata_Cells_Location_Center_X', 'Metadata_Cells_Location_Center_Y']\n",
    "                )\n",
    "            ])\n",
    "\n",
    "    ## Create patch dataset\n",
    "    pds = PatchDataset(\n",
    "        _loaddata_csv=loaddata_condition_df,\n",
    "        _sc_feature=sc_features,\n",
    "        _input_channel_keys=INPUT_CHANNEL_NAMES,\n",
    "        _target_channel_keys=TARGET_CHANNEL_NAMES,\n",
    "        _input_transform=PixelDepthTransform(src_bit_depth=16, target_bit_depth=8, _always_apply=True),\n",
    "        _target_transform=MinMaxNormalize(_normalization_factor=(2 ** 16) - 1, _always_apply=True),\n",
    "        patch_size=PATCH_SIZE,\n",
    "        verbose=False,\n",
    "        patch_generation_method=\"random_cell\",\n",
    "        n_expected_patches_per_img=50,\n",
    "        patch_generation_random_seed=42\n",
    "    )\n",
    "\n",
    "    for channel_name in TARGET_CHANNEL_NAMES:\n",
    "\n",
    "        ## Cache dataset of channel\n",
    "        pds.set_input_channel_keys(INPUT_CHANNEL_NAMES)\n",
    "        pds.set_target_channel_keys(channel_name)\n",
    "        cds = CachedDataset(\n",
    "            dataset=pds,\n",
    "            prefill_cache=True\n",
    "        )\n",
    "\n",
    "        print(f\"Beginning optimization for channel: {channel_name} for {confluence_group_name}\")\n",
    "\n",
    "        # Load the existing study for the current channel\n",
    "        study_path = OPTUNA_JOBLIB_DIR / f\"FNet_optimize_{channel_name}_{confluence_group_name}.joblib\"\n",
    "\n",
    "        if study_path.exists():\n",
    "            study = joblib.load(study_path)\n",
    "            completed_trials = len(study.trials)  # completed trials\n",
    "        else:\n",
    "            study = optuna.create_study(\n",
    "                direction=\"minimize\",\n",
    "                study_name=f\"FNet_optimize_{channel_name}_{confluence_group_name}\",\n",
    "                sampler=optuna.samplers.TPESampler(seed=42)\n",
    "            )\n",
    "            completed_trials = 0  # New experiment\n",
    "            \n",
    "        # Resume optimization from where it left off\n",
    "        trials_remaining = N_TRIALS - completed_trials\n",
    "        if trials_remaining > 0:\n",
    "            for _ in range(trials_remaining):\n",
    "                study.optimize(lambda trial: objective(trial, cds, channel_name, confluence_group_name), n_trials=1)\n",
    "                joblib.dump(study, study_path)  # Save study after every trial\n",
    "                print(f\"Saved study after trial {len(study.trials)}/{N_TRIALS}\")\n",
    "\n",
    "        # Print best trial results\n",
    "        print(f\"Best trial for channel {channel_name}:\")\n",
    "        print(f\"  Validation Loss: {study.best_trial.value}\")\n",
    "        print(f\"  Hyperparameters: {study.best_trial.params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN_EXTRA = False\n",
    "# loaddata_df = pd.read_csv(LOADDATA_FILE_PATH)\n",
    "\n",
    "# for confluence_group_name, conditions in DATA_GROUPING.items():\n",
    "\n",
    "#     ## Load dataset\n",
    "#     loaddata_condition_df = loaddata_df.copy()\n",
    "#     for condition, values in conditions.items():\n",
    "#         loaddata_condition_df = loaddata_condition_df[\n",
    "#             loaddata_condition_df[condition].isin(values)\n",
    "#         ]\n",
    "\n",
    "#     sc_features = pd.DataFrame()\n",
    "#     for plate in loaddata_condition_df['Metadata_Plate'].unique():\n",
    "#         sc_features_parquet = SC_FEATURES_DIR / f'{plate}_sc_normalized.parquet'\n",
    "#         if not sc_features_parquet.exists():\n",
    "#             print(f'{sc_features_parquet} does not exist, skipping...')\n",
    "#             continue \n",
    "#         else:\n",
    "#             sc_features = pd.concat([\n",
    "#                 sc_features, \n",
    "#                 pd.read_parquet(\n",
    "#                     sc_features_parquet,\n",
    "#                     columns=['Metadata_Plate', 'Metadata_Well', 'Metadata_Site', 'Metadata_Cells_Location_Center_X', 'Metadata_Cells_Location_Center_Y']\n",
    "#                 )\n",
    "#             ])\n",
    "\n",
    "#     ## Create patch dataset\n",
    "#     pds = PatchDataset(\n",
    "#         _loaddata_csv=loaddata_condition_df,\n",
    "#         _sc_feature=sc_features,\n",
    "#         _input_channel_keys=INPUT_CHANNEL_NAMES,\n",
    "#         _target_channel_keys=TARGET_CHANNEL_NAMES,\n",
    "#         _input_transform=PixelDepthTransform(src_bit_depth=16, target_bit_depth=8, _always_apply=True),\n",
    "#         _target_transform=MinMaxNormalize(_normalization_factor=(2 ** 16) - 1, _always_apply=True),\n",
    "#         patch_size=PATCH_SIZE,\n",
    "#         verbose=False,\n",
    "#         patch_generation_method=\"random_cell\",\n",
    "#         n_expected_patches_per_img=50,\n",
    "#         patch_generation_random_seed=42\n",
    "#     )\n",
    "\n",
    "#     for channel_name in TARGET_CHANNEL_NAMES:\n",
    "\n",
    "#         ## Cache dataset of channel\n",
    "#         pds.set_input_channel_keys(INPUT_CHANNEL_NAMES)\n",
    "#         pds.set_target_channel_keys(channel_name)\n",
    "#         cds = CachedDataset(\n",
    "#             dataset=pds,\n",
    "#             prefill_cache=True\n",
    "#         )\n",
    "\n",
    "#         print(f\"Beginning optimization for channel: {channel_name} for {confluence_group_name}\")\n",
    "\n",
    "#         # Load the existing study for the current channel\n",
    "#         study_path = OPTUNA_JOBLIB_DIR / f\"FNet_optimize_{channel_name}_{confluence_group_name}.joblib\"\n",
    "#         if study_path.exists():\n",
    "#             if RUN_EXTRA:\n",
    "#                 study = joblib.load(study_path)\n",
    "#             else:\n",
    "#                 print(\"Skipping optimization due to existing joblib...\")\n",
    "#                 continue\n",
    "#         else:\n",
    "#             study = optuna.create_study(\n",
    "#                 direction=\"minimize\",\n",
    "#                 study_name=f\"FNet_optimize_{channel_name}_{confluence_group_name}\",\n",
    "#                 sampler=optuna.samplers.TPESampler(seed=42)\n",
    "#             )\n",
    "\n",
    "#         # Optimize the objective function for the current channel\n",
    "#         study.optimize(lambda trial: objective(trial, cds, channel_name, confluence_group_name), n_trials=50)\n",
    "\n",
    "#         # Save the updated study for the current channel\n",
    "#         joblib.dump(study, study_path)\n",
    "\n",
    "#         # Print best trial results\n",
    "#         print(f\"Best trial for channel {channel_name}:\")\n",
    "#         print(f\"  Validation Loss: {study.best_trial.value}\")\n",
    "#         print(f\"  Hyperparameters: {study.best_trial.params}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speckle_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
