{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook accesses the output of the pediatric_cancer_atlas_profiling up to 2.feature_extraction and uses the relevant metadata, single cell feature and qc metrics to generate data splits for training, heldout and evaluation dataset based on cell line, plate and seeding density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import yaml\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pathlib.Path('.').absolute().parent / \"config.yml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define paths to metadata, loaddata csvs and sc features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROFILING_DIR = pathlib.Path(config['paths']['pediatric_cancer_atlas_profiling_path'])\n",
    "DATASPLIT_OUTPUT_DIR = pathlib.Path('.') / 'data_split_loaddata'\n",
    "DATASPLIT_OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "platemap_csv_path = PROFILING_DIR \\\n",
    "    / \"0.download_data\" / \"metadata\" / \"platemaps\"\n",
    "assert platemap_csv_path.exists()\n",
    "\n",
    "loaddata_csv_path = PROFILING_DIR \\\n",
    "    / \"1.illumination_correction\" / \"loaddata_csvs\"\n",
    "assert loaddata_csv_path.exists()\n",
    "\n",
    "qc_path = pathlib.Path('.').absolute() \\\n",
    "    / \"preprocessing_output\" / \"qc_exclusion.csv\"\n",
    "assert qc_path.exists()\n",
    "\n",
    "sc_features_parquet_path = pathlib.Path(config['paths']['sc_features_path'])\n",
    "\n",
    "assert sc_features_parquet_path.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train data split condition and other datasplit parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whether to remove sites with low QC score\n",
    "QC = True\n",
    "\n",
    "# Define columns in loaddata\n",
    "SITE_COLUMN = 'Metadata_Site'\n",
    "WELL_COLUMN = 'Metadata_Well'\n",
    "PLATE_COLUMN = 'Metadata_Plate'\n",
    "\n",
    "# Wells are uniquely identified by the combination of these columns\n",
    "UNIQUE_IDENTIFIERS = [SITE_COLUMN, WELL_COLUMN, PLATE_COLUMN]\n",
    "\n",
    "# Condition for train data (every other condition will be saved for evaluation)\n",
    "TRAIN_CONDITION_KWARGS = {\n",
    "    'cell_line': 'U2-OS',\n",
    "    'platemap_file': 'Assay_Plate1_platemap',\n",
    "    'seeding_density': [1_000, 2_000, 4_000, 8_000, 12_000]\n",
    "}\n",
    "# Conditions are uniquely identified by the combination of keys from TRAIN_CONDITION_KWARGS\n",
    "CONDITIONS = list(TRAIN_CONDITION_KWARGS.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all barcode/platemap metadata and all loaddata csv files and merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10249 sites prior to QC\n",
      "9358 sites after QC\n"
     ]
    }
   ],
   "source": [
    "## Load platemap and well cell line metadata\n",
    "barcode_df = pd.concat([pd.read_csv(f) for f in platemap_csv_path.glob('Barcode_*.csv')])\n",
    "\n",
    "platemap_df = pd.DataFrame()\n",
    "for platemap in barcode_df['platemap_file'].unique():\n",
    "    df = pd.read_csv(platemap_csv_path / f'{platemap}.csv')\n",
    "    df['platemap_file'] = platemap\n",
    "    platemap_df = pd.concat([platemap_df, df])    \n",
    "barcode_platemap_df = pd.merge(barcode_df, platemap_df, on='platemap_file', how='inner')\n",
    "\n",
    "## QC removal\n",
    "remove_sites = pd.read_csv(qc_path)\n",
    "\n",
    "## Load data csvs\n",
    "loaddata_df = pd.concat(\n",
    "    [pd.read_csv(f) for f in loaddata_csv_path.glob('*.csv')], \n",
    "    ignore_index=True)\n",
    "\n",
    "## Merge loaddata with barcode/platemap metadata to map condition to well\n",
    "loaddata_barcode_platemap_df = pd.merge(\n",
    "    barcode_platemap_df.rename(columns={'barcode': PLATE_COLUMN, 'well': WELL_COLUMN}),\n",
    "    loaddata_df,\n",
    "    on=[PLATE_COLUMN, WELL_COLUMN], \n",
    "    how='left')\n",
    "\n",
    "## Perform QC removal\n",
    "if QC:\n",
    "    print(f\"{loaddata_barcode_platemap_df.shape[0]} sites prior to QC\")\n",
    "    # Merge to correctly identify rows to be removed\n",
    "    qc_merge_df = loaddata_barcode_platemap_df.merge(\n",
    "        remove_sites, \n",
    "        on=UNIQUE_IDENTIFIERS, \n",
    "        how='left', \n",
    "        indicator=True\n",
    "        )\n",
    "\n",
    "    # Keep only rows that were NOT found in remove_sites\n",
    "    loaddata_barcode_platemap_df = qc_merge_df[qc_merge_df['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "    print(f\"{loaddata_barcode_platemap_df.shape[0]} sites after QC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "537 sites for train and heldout\n",
      "8821 sites for evaluation\n"
     ]
    }
   ],
   "source": [
    "loaddata_barcode_platemap_train_df = loaddata_barcode_platemap_df.copy()\n",
    "## Filter load data csvs dynamically with CONDITION_KWARGS\n",
    "for k, v in TRAIN_CONDITION_KWARGS.items():\n",
    "    if isinstance(v, list):\n",
    "        loaddata_barcode_platemap_train_df = loaddata_barcode_platemap_train_df[loaddata_barcode_platemap_train_df[k].isin(v)]\n",
    "    else:\n",
    "        loaddata_barcode_platemap_train_df = loaddata_barcode_platemap_train_df[loaddata_barcode_platemap_train_df[k] == v]\n",
    "    if len(loaddata_barcode_platemap_train_df) == 0:\n",
    "        raise ValueError(f'No data found for {k}={v}')\n",
    "print(f\"{loaddata_barcode_platemap_train_df.shape[0]} sites for train and heldout\")\n",
    "\n",
    "loaddata_barcode_platemap_eval_df = loaddata_barcode_platemap_df.loc[\n",
    "    ~loaddata_barcode_platemap_df.index.isin(loaddata_barcode_platemap_train_df.index)\n",
    "]\n",
    "print(f\"{loaddata_barcode_platemap_eval_df.shape[0]} sites for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For each unique condition combation in train/heldout split, hold out one well at random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Condition: {'cell_line': 'U2-OS', 'platemap_file': 'Assay_Plate1_platemap', 'seeding_density': 1000} Heldout well: ['M14'] Train wells: ['M13' 'N13' 'N14']\n",
      "For Condition: {'cell_line': 'U2-OS', 'platemap_file': 'Assay_Plate1_platemap', 'seeding_density': 2000} Heldout well: ['N16'] Train wells: ['M15' 'N15' 'M16']\n",
      "For Condition: {'cell_line': 'U2-OS', 'platemap_file': 'Assay_Plate1_platemap', 'seeding_density': 4000} Heldout well: ['M17'] Train wells: ['N17' 'M18' 'N18']\n",
      "For Condition: {'cell_line': 'U2-OS', 'platemap_file': 'Assay_Plate1_platemap', 'seeding_density': 8000} Heldout well: ['M20'] Train wells: ['M19' 'N19' 'N20']\n",
      "For Condition: {'cell_line': 'U2-OS', 'platemap_file': 'Assay_Plate1_platemap', 'seeding_density': 12000} Heldout well: ['M22'] Train wells: ['M21' 'N21' 'N22']\n",
      "135 sites Heldout\n",
      "402 sites for Training\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Group by seeding density and cell line\n",
    "grouped = loaddata_barcode_platemap_train_df.groupby(CONDITIONS)\n",
    "\n",
    "# Initialize lists to store holdout and train data\n",
    "heldout_list = []\n",
    "train_list = []\n",
    "\n",
    "# Iterate over each group\n",
    "for _, group in grouped:\n",
    "\n",
    "    held_out_well = [np.random.choice(group[WELL_COLUMN].unique())]\n",
    "    train_wells = group[~group[WELL_COLUMN].isin(held_out_well)][WELL_COLUMN].unique()\n",
    "\n",
    "    loaddata_held_out_df = group[group[WELL_COLUMN].isin(held_out_well)].copy()\n",
    "    loaddata_train_df = group[group[WELL_COLUMN].isin(train_wells)].copy()\n",
    "\n",
    "    condition = group[CONDITIONS].iloc[0].to_dict()\n",
    "    print(f\"For Condition: {condition} Heldout well: {held_out_well} Train wells: {train_wells}\")\n",
    "\n",
    "    heldout_list.append(loaddata_held_out_df)\n",
    "    train_list.append(loaddata_train_df)\n",
    "\n",
    "# Concatenate the lists into dataframes\n",
    "loaddata_heldout_df = pd.concat(heldout_list).reset_index(drop=True)\n",
    "print(f\"{loaddata_heldout_df.shape[0]} sites Heldout\")\n",
    "loaddata_train_df = pd.concat(train_list).reset_index(drop=True)\n",
    "print(f\"{loaddata_train_df.shape[0]} sites for Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaddata_heldout_df.to_csv(DATASPLIT_OUTPUT_DIR / 'loaddata_heldout.csv')\n",
    "loaddata_train_df.to_csv(DATASPLIT_OUTPUT_DIR / 'loaddata_train.csv')\n",
    "loaddata_barcode_platemap_eval_df.to_csv(DATASPLIT_OUTPUT_DIR / 'loaddata_eval.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speckle_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
